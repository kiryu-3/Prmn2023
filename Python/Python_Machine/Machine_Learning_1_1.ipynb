{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMigmRVU9B/F5XCrqNreFWq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiryu-3/Prmn2023/blob/main/Python/Python_Machine/Machine_Learning_1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 重回帰分析ー知識編"
      ],
      "metadata": {
        "id": "3JB9kxUhNfJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 最初にインポートしてください\n",
        "from sklearn.datasets import fetch_openml\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 日本語を利用するためにimport\n",
        "!pip install japanize-matplotlib\n",
        "import japanize_matplotlib"
      ],
      "metadata": {
        "id": "ZXxq2uhI0QIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 説明変数と目的変数"
      ],
      "metadata": {
        "id": "pKXFjOs6R-Bn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「何かの原因となっている変数」を**説明変数**、  \n",
        "「その原因を受けて発生した結果となっている変数」を**目的変数**、  \n",
        "といいます。"
      ],
      "metadata": {
        "id": "f6joGibHSBOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "説明変数と目的変数にはいくつかの表現があります。  \n",
        "詳しくは[こちら](https://bit.ly/3kl6M6S)を参照してください。"
      ],
      "metadata": {
        "id": "gkIsmuOsSWXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 回帰"
      ],
      "metadata": {
        "id": "keRaZCJtS_8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**目的変数**$y$について**説明変数**$x$を使った式で表すことを**回帰**といいます。"
      ],
      "metadata": {
        "id": "clzzn-HKTK4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "例として、身長から体重を予測することを考えてみましょう。\n",
        "\n",
        "身長170cmである場合の平均的な体重を予測しようとしたとき、  \n",
        "「身長が170cm」という条件付きでの体重の平均を，**条件付き平均**といいます。"
      ],
      "metadata": {
        "id": "-FgGGr3NYPFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この**条件付き平均**を以下のような**回帰直線**（**線形回帰**ともいう）を使って求めるのが回帰です。\n",
        "\n",
        "$$\n",
        "\\hat{y}=\\theta_0+\\theta_1x_1+\\theta_2x_2+ … + \\theta_nx_n\n",
        "$$\n",
        "\n",
        "$\\hat{y}$は、$x$に対する$y$の条件付き平均です。"
      ],
      "metadata": {
        "id": "DSftyJTbZNM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 最小二乗法"
      ],
      "metadata": {
        "id": "TrDAvDA8CyjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "パラメータ $\\theta_0 , \\theta_1 , \\theta_2, ･･･ \\theta_n,$ を求めることができれば、  \n",
        "回帰直線が一つに定まります。"
      ],
      "metadata": {
        "id": "R33qQtaaDgF1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "予測値と実測値の差のことを**残差**と呼びます。  \n",
        "（よく聞く「**誤差**」というワードと「**残差**」の違いについては[こちら](https://bit.ly/3KHNXFo)を参照してください。）\n",
        "\n",
        "**残差の二乗和を最小にする**アルゴリズムを**最小二乗法**と呼びます。\n"
      ],
      "metadata": {
        "id": "pUtEiFR4EcJl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "特徴量が一つの場合を例とします。  \n",
        "このとき、回帰直線は以下のように定まるものとします。\n",
        "\n",
        "$$\n",
        "\\hat{f}(x_i) = a + bx_i　(a=\\theta_0 , b=\\theta_1)\n",
        "$$"
      ],
      "metadata": {
        "id": "d24V1rmGGmVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\hat{y}$ は**条件付き平均**、つまり予測値であるので、  \n",
        "実際のデータとは多少の誤差があると考えられます。\n",
        "\n",
        "![リンクテキスト](https://imgur.com/pAMsBS6.png)\n"
      ],
      "metadata": {
        "id": "lWvV1Jare7zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "残差は以下のようにして求めることができます。\n",
        "\n",
        "![リンクテキスト](https://imgur.com/PMQhVAu.png)\n"
      ],
      "metadata": {
        "id": "ACyWZFIvmZ4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**残差の二乗の和**が最小になるような $a$ と$b$ を求めるのが、最小二乗法になります。\n",
        "\n",
        "$$\n",
        "\\sum^{n}_{i=1}{e_i^2}=\\sum^{n}_{i=1}{\\left\\{y_i-(a+bx_i)\\right\\}^2} = \\sum^{n}_{i=1}({y_i-\\hat{f}(x_i))^2}\n",
        "$$\n",
        "\n",
        "上の式のように、「**予測値と実測値のずれを計算する関数**」のことを**損失関数**と呼びます。"
      ],
      "metadata": {
        "id": "N_aQLNZ6ndT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "残差の二乗和を損失関数として使ってもいいんですが、  \n",
        "計算を楽にするために残差の二乗の平均を使うことが多いようです。\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{m}\\sum^{m}_{i=1}({y_i-\\hat{f}(x_i))^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "8dYrO2GsI_lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回の場合であれば後は $a$ と $b$ の値を求めて、  \n",
        "損失関数を「最小化」していくのみとなります。"
      ],
      "metadata": {
        "id": "gQsOiJUbHz-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまでは特徴量が一つの場合を見てきましたが、特徴量が複数になっても基本的な流れは同じです。"
      ],
      "metadata": {
        "id": "6J_i-9oYHSlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 重回帰分析"
      ],
      "metadata": {
        "id": "9lA7t5nYSCIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "複数の説明変数 $x_i$ ($i=1,2,3,…n$)を用いて目的変数$y$を表す回帰分析を  \n",
        "**重回帰分析**といいます。"
      ],
      "metadata": {
        "id": "0XWuTra4SEqs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重回帰分析の流れ"
      ],
      "metadata": {
        "id": "Ck2NBXvjd9Js"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "① **モデル**を決める  \n",
        "② **損失関数**を決める  \n",
        "③ 損失関数を「**最小化**」する"
      ],
      "metadata": {
        "id": "en762NlLeAH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### モデルを決める"
      ],
      "metadata": {
        "id": "YRRY_-OFS6Ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "今回は特徴量が複数なので、各データの値を$x_{ij}$とし、  \n",
        "$j$番目の特徴量の$i$番目のデータという風にします。"
      ],
      "metadata": {
        "id": "v9suKmAsbS1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "予測値 $\\hat{y}_i$ は以下の式で表されます。\n",
        "\n",
        "$$\n",
        "\\hat{y}_i = \\theta_1x_{i1} + \\theta_2x_{i2} + … + \\theta_nx_{in}\n",
        "$$\n",
        "\n",
        "説明変数の重要度が高いものはパラメータ $\\theta$ の値が大きくなります。"
      ],
      "metadata": {
        "id": "CA7oO7TDS8lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\hat{f}(X)=\\theta_0+\\theta_1X_1+\\theta_2X_2+ … + \\theta_nX_n\n",
        "$$"
      ],
      "metadata": {
        "id": "hJIVEpcQgtnm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 損失関数を決める"
      ],
      "metadata": {
        "id": "BT0ZLjxwX0aB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**最小二乗法**によって求めます。   \n",
        "損失関数は残差の二乗の平均を使います。  \n"
      ],
      "metadata": {
        "id": "UnUfTeVzc9w1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\frac{1}{m}\\sum^{m}_{i=1}{e_i^2}=\\frac{1}{m}\\ \\sum^{m}_{i=1}({y_i-\\hat{f}(x_i))^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "-U2dRio3X3eM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 損失関数を「最小化」する"
      ],
      "metadata": {
        "id": "YXPLYaNWd2dF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "パラメータ $\\theta$ は通常、簡単には求めることができません。\n",
        "\n",
        "線形回帰の場合、導出方法は2つほどあります。"
      ],
      "metadata": {
        "id": "82cWszruka68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 最急降下法"
      ],
      "metadata": {
        "id": "TPWttkhCp70r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**最急降下法**は、最も急になる方向に少しずつパラメータを動かして最適解を探る方法です。\n",
        "\n",
        "機械学習ではこのように、パラメータを徐々に最適解に近づけていくことを  \n",
        "**「学習する」**といいます。"
      ],
      "metadata": {
        "id": "jDuibJfOqLD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "回帰式を $\\hat{f}(X)=\\theta_0+\\theta_1X_1+\\theta_2X_2+ … + \\theta_nX_n$ としたとき、  \n",
        "損失関数を以下のように表すとします。\n",
        "\n",
        "$$\n",
        "L(\\theta)=\\frac{1}{m}\\sum^{m}_{i=1}(y_i-\\hat{f}(x_i))^2\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0iE0Krt_sbx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最急降下法のイメージ図は以下のような感じです。  \n",
        "（参考：https://shorturl.at/uS156 ）\n",
        "\n",
        "![](https://imgur.com/EZKpssC.png)"
      ],
      "metadata": {
        "id": "HnGIdaJCxt2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ある地点での接線の傾きの係数は微分で求めることができます。  \n",
        "（参考：https://shorturl.at/ozEZ3 ）\n",
        "\n",
        "![](https://imgur.com/pXDUIIx.png)"
      ],
      "metadata": {
        "id": "2hi-MZBC0czm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれのパラメータを以下の式で同時に更新していきます。\n",
        "\n",
        "$$\n",
        "\\theta_0 := \\theta_0 – \\alpha\\frac{\\partial}{\\partial\\theta_0}L(\\theta)\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_1 := \\theta_1 – \\alpha\\frac{\\partial}{\\partial\\theta_1}L(\\theta)\n",
        "$$\n",
        "\n",
        "\n",
        "$$\n",
        "・\n",
        "$$\n",
        "$$  \n",
        "・  \n",
        "$$\n",
        "$$  \n",
        "・  \n",
        "$$\n",
        "\n",
        "$$\n",
        "\\theta_n := \\theta_n – \\alpha\\frac{\\partial}{\\partial\\theta_n}L(\\theta)\n",
        "$$"
      ],
      "metadata": {
        "id": "0iF_eZsu2AVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "偏微分の結果は以下のようになります。\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_0}L(\\theta)=-\\frac{2}{m}\\sum^{m}_{i=1}(y_i-(\\theta_0+\\theta_1x_{i1} … \\theta_nx_{in}))\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_1}L(\\theta)=-\\frac{2}{m}\\sum^{m}_{i=1}(y_i-(\\theta_0+\\theta_1x_{i1} … \\theta_nx_{in}))x_{i1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\frac{\\partial}{\\partial\\theta_n}L(\\theta)=-\\frac{2}{m}\\sum^{m}_{i=1}(y_i-(\\theta_0+\\theta_1x_{i1} … \\theta_nx_{in}))x_{in}\n",
        "$$\n",
        "\n",
        "※計算過程の詳細について  \n",
        "①カメさんのブログ：https://shorturl.at/bzPY5  \n",
        "②からっぽのしょこさんのブログ：https://www.anarchive-beta.com/entry/2021/08/15/203504"
      ],
      "metadata": {
        "id": "8Rdyfhpu3V17"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 正規方程式"
      ],
      "metadata": {
        "id": "ZbEDljib_g_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "回帰式を $\\hat{f}(X)=\\theta_0+\\theta_1X_1+\\theta_2X_2+ … + \\theta_nX_n$ としたとき、  \n",
        "損失関数を以下のように表すとします。\n",
        "\n",
        "$$\n",
        "L(\\theta)=\\frac{1}{m}\\sum^{m}_{i=1}(y_i-\\hat{f}(x_i))^2\n",
        "$$\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LTFs9CxkBTUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "誤差を考慮する必要があるので、これを $b$（**バイアス**）で表します。\n",
        "\n",
        "式を書き直すと以下のようになります。\n",
        "\n",
        "$$\n",
        "\\hat{f}(x_i) = \\theta_1x_{i1} + \\theta_2x_{i2} + … + \\theta_nx_{in} + b\n",
        "$$"
      ],
      "metadata": {
        "id": "cPfn-PBMUjiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$b = \\theta_0×x_0 = \\theta_0×1$ とダミーの変数を用いることで、  \n",
        "形式を他の項に合わせることができます。\n",
        "\n",
        "$$\n",
        "\\hat{f}(x_i) = \\theta_1x_{i1} + \\theta_2x_{i2} + … + \\theta_nx_{in} + \\theta_0x_{i0}\n",
        "$$"
      ],
      "metadata": {
        "id": "Puo60nX5VAU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これを行列の形に変形します。\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}_1 \\\\\n",
        "\\hat{y}_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}_{m-1} \\\\\n",
        "\\hat{y}_m \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "x_{10} & x_{11} & \\cdots & x_{1n-1} & x_{1n}   \\\\\n",
        "x_{20} & x_{21} & \\cdots & x_{2n-1} & x_{2n}   \\\\\n",
        "\\vdots & \\vdots & \\cdots & \\vdots & \\vdots \\\\\n",
        "x_{m-10} & x_{m-11} & \\cdots & x_{m-1n-1} & x_{m-1n}   \\\\\n",
        "x_{m0} & x_{m1} & \\cdots & x_{mn-1} & x_{mn}   \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\theta_0 \\\\\n",
        "\\theta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\vdots \\\\\n",
        "\\theta_{n-1} \\\\\n",
        "\\theta_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "soGcnj9rDUmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これを行列の形に変形します。\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "\\hat{y}_1 \\\\\n",
        "\\hat{y}_2 \\\\\n",
        "\\vdots \\\\\n",
        "\\hat{y}_{m-1} \\\\\n",
        "\\hat{y}_m \\\\\n",
        "\\end{bmatrix}\n",
        "=\n",
        "\\begin{bmatrix}\n",
        "x_{10} & x_{11} & \\cdots & x_{1n-1} & x_{1n}   \\\\\n",
        "x_{20} & x_{21} & \\cdots & x_{2n-1} & x_{2n}   \\\\\n",
        "\\vdots & \\vdots & \\cdots & \\vdots & \\vdots \\\\\n",
        "x_{m-10} & x_{m-11} & \\cdots & x_{m-1n-1} & x_{m-1n}   \\\\\n",
        "x_{m0} & x_{m1} & \\cdots & x_{mn-1} & x_{mn}   \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "\\theta_0 \\\\\n",
        "\\theta_1 \\\\\n",
        "\\vdots \\\\\n",
        "\\vdots \\\\\n",
        "\\theta_{n-1} \\\\\n",
        "\\theta_n \\\\\n",
        "\\end{bmatrix}\n",
        "$$"
      ],
      "metadata": {
        "id": "794yVgO5Vzos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "それぞれの行列およびベクトルを $\\mathbf{\\hat{y}},\\mathbf{X},\\theta$ とすると、\n",
        "\n",
        "$$\n",
        "\\mathbf{\\hat{y}}=\\mathbf{X}\\mathbf{\\theta}\n",
        "$$\n",
        "\n",
        "と表すことができます。"
      ],
      "metadata": {
        "id": "WUF3r8XHek-r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失関数は以下のように計算できます。\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\sum^{m}_{i=1}(y_i-\\hat{y_i})^2&=(\\mathbf{y}-\\mathbf{X}\\mathbf{\\theta})^T(\\mathbf{y}-\\mathbf{X}\\mathbf{\\theta})\\\\\n",
        "&=(\\mathbf{y}^T-\\mathbf{\\theta}^T\\mathbf{X}^T)(\\mathbf{y}-\\mathbf{X}\\mathbf{\\theta})\\\\\n",
        "&=\\mathbf{y}^T\\mathbf{y}-2\\mathbf{\\theta}^T\\mathbf{X}^T\\mathbf{y}+\\mathbf{\\theta}^T\\mathbf{X}^T\\mathbf{X}\\theta\n",
        "\\end{align*}\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "d2wyIvV6GgUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これを$\\theta$で偏微分して、0になる$\\theta$が最適解になります。\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "-2\\mathbf{X}^T\\mathbf{y}+2\\mathbf{X}^T\\mathbf{X}\\theta=0\n",
        "\\end{align*}\n",
        "$$"
      ],
      "metadata": {
        "id": "syWLSRS-G3in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失関数を計算すると、以下のような結果が得られます。\n",
        "\n",
        "$$\n",
        "\\theta=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\n",
        "$$\n",
        "\n",
        "（答えの導出過程は[こちら](https://bit.ly/3lTzKuL) を参照してください。)"
      ],
      "metadata": {
        "id": "GU3v_ZpLHmXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 最急降下法と正規方程式"
      ],
      "metadata": {
        "id": "C_qSFqyoIAz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "一発の数式でパッと解を出せるのが**正規方程式**の大きな特徴です。\n",
        "\n",
        "しかし、特徴量の数$n$が多すぎると$(\\mathbf{X}^T\\mathbf{X})^{-1}$の計算に時間がかかります。  \n",
        "計算量は$n^3$のペースで増えていくようです。"
      ],
      "metadata": {
        "id": "aecf1V6JIG5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$n$が大きい場合（1万とか？）は、**最急降下法**を使って解くことが一般的なようです。"
      ],
      "metadata": {
        "id": "tB0aXNTrJSty"
      }
    }
  ]
}